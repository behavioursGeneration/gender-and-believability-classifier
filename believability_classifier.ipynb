{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from utils.utils import *\n",
    "from utils.get_data import load_data\n",
    "from utils.get_generated_data import load_generated_data\n",
    "from math import floor\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from os.path import join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieur on believability for non verbal behaviour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class Discriminator(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ##encode audio\n",
    "        self.conv1_audio = Conv(1025, 512)\n",
    "        self.conv2_audio = Conv(512, 128)\n",
    "        self.conv3_audio = Conv(128, 64)\n",
    "\n",
    "        ##encode behaviour\n",
    "        self.conv1_behaviour = Conv(28, 32)\n",
    "        self.conv2_behaviour = Conv(32, 64)\n",
    "\n",
    "        self.conv_concat = Conv(128, 64)\n",
    "        self.fc1 = torch.nn.Linear(64 * floor(100/4), 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 1)\n",
    "    \n",
    "\n",
    "    def forward(self, x_pose, c_audio):\n",
    "        in_audio = torch.swapaxes(c_audio, 1, 2)\n",
    "        c = self.conv1_audio(in_audio)\n",
    "        c = F.max_pool1d(c, kernel_size=2, stride=2)\n",
    "        c = self.conv2_audio(c)\n",
    "        c = F.max_pool1d(c, kernel_size=2, stride=2)\n",
    "        c = self.conv3_audio(c)\n",
    "        c = F.max_pool1d(c, kernel_size=2, stride=2)\n",
    "\n",
    "        x = torch.swapaxes(x_pose, 1, 2)\n",
    "        x = self.conv1_behaviour(x)\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "        x = self.conv2_behaviour(x)\n",
    "        x = F.max_pool1d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        x = self.conv_concat(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supress_index(raw_labels_list, labels_list, x_audio, x_behaviour):\n",
    "    indices_a_supprimer = []\n",
    "    for i in range(len(raw_labels_list)):\n",
    "        if \"silence\" in raw_labels_list[i]:\n",
    "            indices_a_supprimer.append(i)\n",
    "\n",
    "    tensor_audio = x_audio.clone()\n",
    "    tensor_behaviour = x_behaviour.clone()\n",
    "    masque = torch.ones(x_audio.size(0), dtype=torch.bool)\n",
    "    masque[indices_a_supprimer] = False\n",
    "    tensor_audio_without_silence_index = torch.index_select(tensor_audio, dim=0, index=torch.nonzero(masque).squeeze())\n",
    "    tensor_behaviour_without_silence_index = torch.index_select(tensor_behaviour, dim=0, index=torch.nonzero(masque).squeeze())\n",
    "    new_labels_list = torch.index_select(labels_list, dim=0, index=torch.nonzero(masque).squeeze())\n",
    "    return tensor_audio_without_silence_index, tensor_behaviour_without_silence_index, new_labels_list\n",
    "\n",
    "def reshape_behaviour_for_classif(x_audio, y_behaviour, scaler=None):\n",
    "    minMaxScaler = MinMaxScaler((-1,1))\n",
    "    if scaler is None:\n",
    "        scaler = minMaxScaler.fit(y_behaviour.view(-1, y_behaviour.size()[2])) \n",
    "    y_scaled = torch.empty(size=(y_behaviour.size()[0], y_behaviour.size()[1], y_behaviour.size()[2]))\n",
    "    for i in range(y_behaviour.size()[0]):\n",
    "        y_scaled[i] = torch.tensor(scaler.transform(y_behaviour[i])) \n",
    "    return x_audio, y_scaled, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Discriminator()\n",
    "checkpoint = torch.load(\"saved_models/believability_classifier.ckpt\", map_location=torch.device('cpu'))['state_dict']\n",
    "for key in list(checkpoint.keys()):\n",
    "    if 'discriminator.' in key:\n",
    "        checkpoint[key.replace('discriminator.', '')] = checkpoint[key]\n",
    "        del checkpoint[key]\n",
    "    else:\n",
    "        del checkpoint[key]\n",
    "print(checkpoint.keys())\n",
    "classifier.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = load_data(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict[\"test_generated\"] = load_generated_data(\"25-04-2024_trueness_1_CGAN_17/epoch_450\", create_init_files=True)\n",
    "\n",
    "def trouver_tous_index(tableau, valeur):\n",
    "    indexes = [i for i, x in enumerate(tableau) if x == valeur]\n",
    "    return set(indexes)\n",
    "\n",
    "dict[\"test_generated\"][\"X_audio_hubert\"] = []\n",
    "\n",
    "for i in range(len(dict[\"test_generated\"][\"keys\"])):\n",
    "    key = dict[\"test_generated\"][\"keys\"][i]\n",
    "    interval = dict[\"test_generated\"][\"interval\"][i]\n",
    "    index_interval = trouver_tous_index(dict[\"test\"][\"interval\"],interval)\n",
    "    index_key = trouver_tous_index(dict[\"test\"][\"keys\"],key)\n",
    "    index = index_interval.intersection(index_key)\n",
    "    dict[\"test_generated\"][\"X_audio_hubert\"].append(dict[\"test\"][\"X_audio_hubert\"][list(index)[0]])\n",
    "dict[\"test_generated\"][\"X_audio_hubert\"] = torch.stack(dict[\"test_generated\"][\"X_audio_hubert\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the discriminator from the trained model of behaviour generation\n",
    "\n",
    "this classifier/discriminator of generated vs. real behavior is trained on a GAN-type model of non-verbal behavior generation. The discriminator parameters of this GAN are extracted and used here to discriminate the believability of behavior files. \n",
    "\n",
    "We get 1 if the classifier thinks the behavior comes from a real behavior and 0 if the classifier thinks the behavior comes from a generated behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = \"gender\"\n",
    "tensor = \"one_hot_tensor_gender\"\n",
    "nb_labels = 3\n",
    "test = \"test\"\n",
    "\n",
    "test_data_audio, test_data_behaviour, test_labels = supress_index(dict[test][label], dict[test][tensor], dict[test][\"X_audio_hubert\"], dict[test][\"Y_behaviour\"])\n",
    "y_scaler = pickle.load(open(join(\"saved_models\", 'scaler_y.pkl'), 'rb'))\n",
    "X_test, y_test, _ = reshape_behaviour_for_classif(test_data_audio, test_data_behaviour, y_scaler)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    for audio, behaviour in test_loader:\n",
    "        prediction = classifier(behaviour, audio)\n",
    "        all_predictions.extend(prediction)\n",
    "print(torch.mean(torch.stack(all_predictions)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
